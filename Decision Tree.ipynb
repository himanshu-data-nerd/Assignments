{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "812cb73b",
   "metadata": {},
   "source": [
    "\n",
    "## Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A Decision Tree is a supervised machine learning model that maps observations about an item to conclusions about its target value. In classification tasks, the target is a discrete label (e.g., species of iris, disease vs no-disease). The structure of a decision tree resembles a flowchart: internal nodes represent tests on features (e.g., \"sepal length > 5.5?\"), branches represent outcomes of the test, and leaf nodes represent class labels or class distributions.\n",
    "\n",
    "**How it works (step-by-step):**\n",
    "1. **Root node creation:** The algorithm examines all features and possible splits on those features to find the split that best separates the classes according to a chosen impurity measure (like Gini or Entropy).\n",
    "2. **Recursive splitting:** Once the best split is chosen, the dataset is partitioned into subsets — one per branch — and the process repeats on each subset. This recursion continues until a stopping criterion is met (pure nodes, minimal samples, or maximum depth).\n",
    "3. **Leaf prediction:** When splitting stops, each leaf node holds the class most frequent among the training samples that reached that leaf. For probabilistic predictions, the leaf can provide class probabilities based on the class distribution at that leaf.\n",
    "4. **Prediction for new samples:** To classify a new instance, traverse the tree from the root, following the branch whose condition matches the instance, until a leaf is reached. The leaf’s class (or probabilities) become the model’s prediction.\n",
    "\n",
    "**Key properties and intuition:**\n",
    "- Decision trees are **non-parametric**: they make no assumption about the data distribution.\n",
    "- They are **interpretable**: the sequence of decisions (feature thresholds) leading to a prediction can be easily visualized and explained.\n",
    "- Trees can **handle both numerical and categorical** features (categorical handling may require encoding depending on implementation).\n",
    "- They are **invariant to monotonic transformations** of features (e.g., log scaling) because splits depend on orderings and thresholds.\n",
    "\n",
    "**When they shine:** When interpretability matters, when relationships between features and outcomes are hierarchical or piecewise-constant, and when mixed-type features are present.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714abc4",
   "metadata": {},
   "source": [
    "\n",
    "## Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Decision trees use impurity measures to evaluate how well a split separates the classes. Two common measures are **Gini Impurity** and **Entropy (Information Entropy)**.\n",
    "\n",
    "**Gini Impurity**\n",
    "- For a node with class probabilities \\(p_1, p_2, ..., p_k\\), Gini impurity is defined as:  \n",
    "  $$ G = 1 - \\sum_{i=1}^{k} p_i^2 $$\n",
    "- Interpretation: it is the probability that a randomly chosen sample from the node would be misclassified if it were labeled randomly according to the class distribution in that node.\n",
    "- Range: \\(0\\) (pure node) to \\((1 - 1/k)\\) for k classes (max impurity when classes are uniformly distributed).\n",
    "\n",
    "**Entropy**\n",
    "- Entropy is defined as:  \n",
    "  $$ H = -\\sum_{i=1}^{k} p_i \\log_2 p_i $$\n",
    "- Interpretation: it quantifies the uncertainty or disorder in the class distribution. A pure node has entropy 0; a maximally mixed node has higher entropy.\n",
    "- Entropy is rooted in information theory and measures the expected number of bits needed to encode the class label.\n",
    "\n",
    "**Impact on splits**\n",
    "- At each candidate split, the algorithm computes the **weighted impurity** of the child nodes and chooses the split that **minimizes** the weighted impurity (equivalently, maximizes impurity reduction).\n",
    "- **Information Gain** is often used with entropy; it measures the decrease in entropy after the split. With Gini, the analogous concept is Gini gain (impurity reduction).\n",
    "- In practice, Gini and Entropy often lead to similar trees. Gini is slightly faster to compute and tends to isolate the most frequent class in a node, while entropy can be marginally more sensitive to changes in class probabilities.\n",
    "- Choice of impurity can slightly affect which feature is chosen at nodes, but the overall predictive performance is typically comparable. For most tasks, using the library default (Gini in many implementations) is acceptable unless fine-grained control or interpretability by information-theoretic reasoning is desired.\n",
    "\n",
    "**Practical note:** When comparing splits, always consider not just immediate impurity reduction but also downstream effects. Cross-validation or validation sets help ensure chosen impurity and splitting strategy generalize well.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f997ebcf",
   "metadata": {},
   "source": [
    "\n",
    "## Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Decision trees can easily overfit the training data by creating very deep trees that capture noise. Pruning strategies control complexity and improve generalization.\n",
    "\n",
    "**Pre-Pruning (Early Stopping)**\n",
    "- **What it is:** Pre-pruning stops the tree-growing process early by imposing constraints during training. Examples: setting `max_depth`, `min_samples_split`, `min_samples_leaf`, or `max_leaf_nodes`.\n",
    "- **Mechanism:** During recursive splitting, the algorithm checks stopping criteria; if they are met, it refrains from splitting further even if impurity can still be reduced.\n",
    "- **Practical advantage:** **Computational efficiency and simplicity.** Pre-pruning reduces model training time and memory because the tree never grows beyond specified limits. It is straightforward to implement and tune when you have limited computational resources or when you want an immediately interpretable shallow tree.\n",
    "\n",
    "**Post-Pruning (Prune After Full Growth)**\n",
    "- **What it is:** Post-pruning first grows a full (or very large) tree, then prunes back branches that do not provide sufficient predictive power on validation data. Methods include reduced-error pruning and cost-complexity pruning (a.k.a. weakest link pruning or CCP).\n",
    "- **Mechanism:** Evaluate subtrees using validation set or by optimizing a complexity-penalized objective (e.g., minimize training loss + alpha * number_of_leaves). Remove branches that increase generalization error or do not justify their complexity.\n",
    "- **Practical advantage:** **Often yields better generalization.** Post-pruning examines the actual contribution of branches and can remove splits that only fit noise. It tends to produce a simpler tree without prematurely discarding potentially useful structure.\n",
    "\n",
    "**Summary comparison**\n",
    "- Pre-pruning is faster and simpler but risks underfitting if constraints are too strict.\n",
    "- Post-pruning is more thorough and often achieves better bias-variance trade-offs but requires extra computation (validation set or cross-validation) and is more complex to implement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee460ab3",
   "metadata": {},
   "source": [
    "\n",
    "## Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Definition:** Information Gain (IG) is a metric that quantifies the reduction in impurity (uncertainty) achieved by partitioning a dataset based on a particular feature. In the entropy framework:\n",
    "\n",
    "$$\n",
    "IG(parent, split) = H(parent) - \\sum_{children} \f",
    "rac{N_{child}}{N_{parent}} H(child)\n",
    "$$\n",
    "\n",
    "where \\(H\\) is entropy and \\(N\\) the number of samples.\n",
    "\n",
    "**Why it matters:**\n",
    "1. **Quantifies usefulness of a split:** IG measures how well a given feature separates the classes. The higher the IG, the more informative the split.\n",
    "2. **Guides greedy selection:** Decision tree algorithms use IG (or analogous impurity reductions like Gini gain) to greedily pick the best split at each node. This local decision-making is what builds the hierarchical structure of the tree.\n",
    "3. **Balances purity and partition size:** Because IG uses weighted child entropies, it accounts for both the purity of child nodes and the number of samples in each child. A split that isolates a tiny pure subset but leaves a large impure remainder may not have as high IG as a balanced split.\n",
    "4. **Interpretability and feature importance:** Summed or averaged IG across splits involving a feature can provide an estimate of that feature’s importance in the learned tree.\n",
    "\n",
    "**Limitations of Information Gain:**\n",
    "- IG can be biased toward features with many possible values (e.g., ID-like features). To counteract this, alternatives like **Information Gain Ratio** (used in C4.5) adjust IG by split information.\n",
    "- IG is a *greedy* local measure and does not consider future splits; hence, the globally optimal tree may not be achieved.\n",
    "\n",
    "In practice, IG is a principled and widely-used criterion that transforms the abstract goal of improved predictability into a quantifiable objective during tree construction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5db552",
   "metadata": {},
   "source": [
    "\n",
    "## Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Applications**\n",
    "- **Healthcare & Medical Diagnosis:** Predict disease presence from symptoms, lab tests; decision trees map well to clinical decision rules.\n",
    "- **Finance & Credit Scoring:** Determine whether to approve loans using applicant features (income, employment history).\n",
    "- **Marketing & Customer Segmentation:** Classify customers likely to churn or respond to campaigns.\n",
    "- **Fraud Detection:** Spotlight transactions that are likely fraudulent based on transaction features.\n",
    "- **Manufacturing & Quality Control:** Classify defective vs non-defective products using sensor measurements.\n",
    "- **Rule Extraction & Compliance:** Derive explicit decision rules for regulatory audits.\n",
    "\n",
    "**Advantages**\n",
    "1. **Interpretability:** Trees provide if-then rules that stakeholders (including non-technical) can understand.\n",
    "2. **No need for heavy preprocessing:** They handle mixed feature types, do not require feature scaling, and can work with missing values (in some implementations).\n",
    "3. **Fast prediction:** Once trained, tree traversal is efficient and predictable in latency.\n",
    "4. **Implicit feature selection:** Important features appear near the root; uninformative features are rarely used.\n",
    "\n",
    "**Limitations**\n",
    "1. **Overfitting risk:** Trees can grow deep and model noise. Pruning and parameter tuning are needed to avoid overfitting.\n",
    "2. **Instability:** Small changes in data can yield very different trees (high variance).\n",
    "3. **Bias toward dominant classes or features:** Without balancing or careful splitting criteria, trees can favor majority classes.\n",
    "4. **Greedy learning:** The top-down greedy split selection may miss globally optimal trees.\n",
    "5. **Poor extrapolation for regression:** Tree-based regression predicts piecewise-constant values and may be less smooth than parametric models.\n",
    "\n",
    "**Mitigations**\n",
    "- Use ensembles (Random Forests, Gradient Boosting) to reduce variance and improve accuracy while retaining interpretability through feature importance and partial dependence plots.\n",
    "- Apply pruning, cross-validation, and domain-informed feature engineering to build robust trees.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ecd1d",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "Load the Iris dataset, train a Decision Tree Classifier using the Gini criterion, and print the model's accuracy and feature importances.\n",
    "\n",
    "We'll load Iris, split into train/test, train a DecisionTreeClassifier(criterion='gini'), and report accuracy and feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442ba048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9333\n",
      "Feature importances:\n",
      "  sepal length (cm): 0.0000\n",
      "  sepal width (cm): 0.0286\n",
      "  petal length (cm): 0.5412\n",
      "  petal width (cm): 0.4303\n"
     ]
    }
   ],
   "source": [
    "# Question 6 - Code\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Train Decision Tree with Gini\n",
    "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "print(\"Feature importances:\")\n",
    "for name, importance in zip(data.feature_names, model.feature_importances_):\n",
    "    print(f\"  {name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0c1bd",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "Train a Decision Tree Classifier with `max_depth=3` and compare its accuracy to a fully-grown tree.\n",
    "\n",
    "A shallow tree (max_depth=3) often generalizes better; compare test accuracies to observe overfitting in the full tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523b230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full tree test accuracy: 0.9333\n",
      "Max-depth=3 tree test accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "# Question 7 - Code\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Full (unrestricted) tree\n",
    "full_tree = DecisionTreeClassifier(random_state=42)\n",
    "full_tree.fit(X_train, y_train)\n",
    "full_acc = accuracy_score(y_test, full_tree.predict(X_test))\n",
    "\n",
    "# Restricted tree\n",
    "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "limited_tree.fit(X_train, y_train)\n",
    "limited_acc = accuracy_score(y_test, limited_tree.predict(X_test))\n",
    "\n",
    "print(f\"Full tree test accuracy: {full_acc:.4f}\")\n",
    "print(f\"Max-depth=3 tree test accuracy: {limited_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791cb32",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "Load the California Housing dataset, train a Decision Tree Regressor, and print the Mean Squared Error (MSE) and feature importances.\n",
    "\n",
    "`load_boston` is deprecated/removed; we use `fetch_california_housing()` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b1b304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.5280\n",
      "Feature importances:\n",
      "  MedInc: 0.5235\n",
      "  HouseAge: 0.0521\n",
      "  AveRooms: 0.0494\n",
      "  AveBedrms: 0.0250\n",
      "  Population: 0.0322\n",
      "  AveOccup: 0.1390\n",
      "  Latitude: 0.0900\n",
      "  Longitude: 0.0888\n"
     ]
    }
   ],
   "source": [
    "# Question 8 - Code\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "california = fetch_california_housing()\n",
    "Xb, yb = california.data, california.target\n",
    "\n",
    "# Split data (re-using train_test_split imported earlier)\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(Xb, yb, test_size=0.3, random_state=42)\n",
    "\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "reg.fit(Xb_train, yb_train)\n",
    "\n",
    "pred_b = reg.predict(Xb_test)\n",
    "mse = mean_squared_error(yb_test, pred_b)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(\"Feature importances:\")\n",
    "for name, importance in zip(california.feature_names, reg.feature_importances_):\n",
    "    print(f\"  {name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97d81b",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "Tune the Decision Tree's `max_depth` and `min_samples_split` using GridSearchCV and print the best parameters and resulting model accuracy.\n",
    "\n",
    "We'll perform a small grid search with cross-validation on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6188d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
      "Best cross-validation accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "# Question 9 - Code\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "params = {\n",
    "    'max_depth': [2,3,4,5,None],\n",
    "    'min_samples_split': [2,3,4,5]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(f\"Best cross-validation accuracy: {grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87359d10",
   "metadata": {},
   "source": [
    "\n",
    "## Question 10: Practical workflow for a healthcare classification problem (predicting disease)  \n",
    "**Provide detailed step-by-step process to handle missing values, encode categorical features, train a Decision Tree model, tune hyperparameters, evaluate performance, and describe business value. (10 marks)**\n",
    "\n",
    "**1. Problem understanding & data audit**  \n",
    "- Identify target variable (disease: yes/no) and features (demographics, vitals, labs, imaging metadata).  \n",
    "- Check class balance, typical ranges, missingness patterns, and data types. Visualize distributions and correlations.\n",
    "\n",
    "**2. Handle missing values**  \n",
    "- **Quantify missingness:** compute missingness per column and by patient.  \n",
    "- **Missing completely at random (MCAR) vs MAR vs MNAR:** attempt to diagnose the mechanism. If missingness correlates with target or other features, be careful.  \n",
    "- **Imputation strategies:**  \n",
    "  - Numerical: use domain-aware imputation (median robust to outliers, or KNN/imputation with other correlated features).  \n",
    "  - Categorical: impute with a separate category (\"Missing\") or mode; consider learning-based imputation for complex patterns.  \n",
    "  - If a feature has extremely high missing rate (e.g., >60–80%) consider dropping it unless clinically important.  \n",
    "  - Preserve missingness indicators (binary flag) for features where missingness itself is informative.\n",
    "\n",
    "**3. Encode categorical features**  \n",
    "- For Decision Trees, **label encoding** can work because splits can handle ordinal thresholds; but beware: for nominal categories, label encoding may introduce arbitrary ordinality — however, tree algorithms typically handle that fine because they split on equality checks implicitly.  \n",
    "- **One-Hot Encoding** is safe and interpretable, but increases dimensionality; use it for low-cardinality features.  \n",
    "- For high-cardinality categorical features, consider target encoding or frequency encoding with appropriate cross-validation to avoid leakage.\n",
    "\n",
    "**4. Feature engineering & scaling**  \n",
    "- Trees don't require feature scaling; normalization is not necessary.  \n",
    "- Create clinically-relevant features (ratios, flags, binned ages), interaction terms if meaningful, and temporal aggregations for time-series data.\n",
    "\n",
    "**5. Train/Test split and cross-validation**  \n",
    "- Use stratified split to maintain class balance in train/test.  \n",
    "- Use K-fold or stratified K-fold cross-validation during model selection to estimate generalization performance reliably.\n",
    "\n",
    "**6. Train Decision Tree model**  \n",
    "- Start with a baseline DecisionTreeClassifier (no or mild constraints). Use `class_weight='balanced'` if classes are imbalanced.  \n",
    "- Monitor training vs validation performance to detect overfitting.\n",
    "\n",
    "**7. Hyperparameter tuning**  \n",
    "- Tune `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features`, and `ccp_alpha` (cost-complexity pruning).  \n",
    "- Use `GridSearchCV` or `RandomizedSearchCV` with stratified folds and scoring metrics aligned to business needs (e.g., `f1`, `recall` for disease detection).  \n",
    "- Use pipeline objects (`sklearn.pipeline.Pipeline`) to chain preprocessing (imputation, encoding) and model training to avoid leakage.\n",
    "\n",
    "**8. Evaluation metrics**  \n",
    "- For healthcare, **sensitivity/recall** (catching true positives) and **specificity** are crucial. Also report **precision**, **F1-score**, **ROC AUC**, and confusion matrix.  \n",
    "- Use calibration plots (probability calibration) because predicted probabilities may be used for risk stratification.  \n",
    "- If costs differ (false negative more costly than false positive), incorporate them into thresholding or use cost-sensitive learning.\n",
    "\n",
    "**9. Model validation and robustness checks**  \n",
    "- Validate on an external hold-out or temporal split to assess performance over time.  \n",
    "- Perform subgroup analysis (different age groups, genders, clinical sites) to ensure fairness.  \n",
    "- Conduct permutation importance and SHAP analysis for interpretability and to detect spurious associations.\n",
    "\n",
    "**10. Deployment and monitoring**  \n",
    "- Export model along with preprocessing pipeline. Validate on live data before full deployment.  \n",
    "- Monitor model drift and recalibrate or retrain periodically. Track performance metrics and data distribution shifts.\n",
    "\n",
    "**11. Ethical, legal and privacy considerations**  \n",
    "- Ensure compliance with healthcare regulations (HIPAA/GDPR equivalents).  \n",
    "- Document model limitations, perform bias audits, and keep clinicians in the loop for acceptance.\n",
    "\n",
    "**Business value:**  \n",
    "A reliable disease-prediction model can enable early detection, prioritize high-risk patients for timely intervention, reduce unnecessary tests, optimize resource allocation, and improve patient outcomes. It also supports clinicians with decision support, potentially lowering costs and improving throughput when integrated into clinical workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22dca18-29f1-4fbe-bc4a-10a2d7a9be3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
