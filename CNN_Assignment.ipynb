{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce10f09",
   "metadata": {},
   "source": [
    "**Question 1:** What is the role of filters and feature maps in Convolutional Neural Networks (CNN)?\n",
    "\n",
    "**Answer:**  \n",
    "In Convolutional Neural Networks (CNNs), filters (also called kernels) and feature maps are the fundamental components responsible for automatic feature extraction from input data, particularly images. A filter is a small matrix of learnable parameters that slides over the input image and performs an element-wise multiplication followed by summation. Each filter is designed to detect a specific pattern such as edges, corners, textures, or more complex shapes.\n",
    "\n",
    "When a filter convolves with the input, the result is a **feature map**. A feature map represents the spatial locations where a particular feature is detected. For example, an edge-detection filter will produce high values in the feature map where edges are present in the image. Multiple filters are used in each convolutional layer, resulting in multiple feature maps, each capturing a different type of feature.\n",
    "\n",
    "The hierarchical nature of CNNs allows early layers to learn low-level features like edges and gradients, while deeper layers learn high-level semantic features such as objects or shapes. This automatic feature learning eliminates the need for manual feature engineering, which is a major advantage of CNNs over traditional machine learning approaches.\n",
    "\n",
    "Additionally, filters are shared across the entire image, which drastically reduces the number of parameters compared to fully connected networks. This weight sharing makes CNNs computationally efficient and helps them generalize better. Feature maps preserve spatial relationships, enabling CNNs to understand the structure and composition of images effectively. Overall, filters and feature maps are the core mechanisms that enable CNNs to learn rich and meaningful representations from visual data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790a351",
   "metadata": {},
   "source": [
    "**Question 2:**  Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?\n",
    "\n",
    "\n",
    "**Answer:**  \n",
    "Padding and stride are two critical hyperparameters in convolutional neural networks that control how convolution operations are applied to input data. **Padding** refers to adding extra pixels (usually zeros) around the border of the input image before applying the convolution. The main purpose of padding is to control the spatial size of the output feature map and to preserve edge information. Without padding, convolution reduces the size of the image, which may result in loss of important boundary features. Common padding types include *valid padding* (no padding) and *same padding* (output size equals input size).\n",
    "\n",
    "**Stride** defines how many pixels the filter moves at each step while sliding across the input. A stride of 1 means the filter moves one pixel at a time, resulting in a larger output feature map. A larger stride reduces the spatial dimensions of the output and acts as a form of downsampling.\n",
    "\n",
    "The output size of a convolution operation is determined by the formula:  \n",
    "\\[ Output = \\frac{(N - F + 2P)}{S} + 1 \\]  \n",
    "where N is input size, F is filter size, P is padding, and S is stride.\n",
    "\n",
    "Padding increases output dimensions, while stride decreases them. Together, they control computational cost, feature resolution, and model performance. Proper selection of padding and stride ensures efficient learning without losing critical spatial information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006068a",
   "metadata": {},
   "source": [
    "**Question 3:** Define receptive field in the context of CNNs. Why is it important for deep architectures?\n",
    "\n",
    "**Answer:**  \n",
    "The receptive field in a Convolutional Neural Network refers to the region of the input image that influences the activation of a particular neuron in a feature map. In simpler terms, it defines how much of the original image a neuron can “see.” In the first convolutional layer, the receptive field is equal to the filter size. However, as we move deeper into the network, the receptive field grows due to stacking of convolutional and pooling layers.\n",
    "\n",
    "The importance of the receptive field lies in its ability to capture contextual information. Small receptive fields focus on local features such as edges and textures, while larger receptive fields enable neurons to capture global structures and object-level information. Deep architectures allow CNNs to gradually expand the receptive field without increasing filter size, which improves efficiency.\n",
    "\n",
    "A properly designed receptive field ensures that neurons in deeper layers can integrate information from larger portions of the image, which is crucial for tasks like object detection and medical image analysis. If the receptive field is too small, the network may fail to capture long-range dependencies. If too large too early, it may lose fine details. Thus, receptive field design directly impacts learning quality and performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce45bae",
   "metadata": {},
   "source": [
    "**Question 4:**  Discuss how filter size and stride influence the number of parameters in a CNN.\n",
    "\n",
    "**Answer:**  \n",
    "The number of parameters in a CNN is primarily determined by the filter size, number of filters, and input depth. A convolutional filter has parameters equal to (filter height × filter width × input channels). Increasing the filter size directly increases the number of parameters, leading to higher computational cost and risk of overfitting.\n",
    "\n",
    "Stride, on the other hand, does not directly affect the number of parameters but influences the size of the output feature map. Larger strides reduce spatial dimensions, which indirectly reduces the number of activations and computations in subsequent layers.\n",
    "\n",
    "Using smaller filters (e.g., 3×3) stacked in deeper networks is often preferred over large filters (e.g., 7×7) because it reduces parameters while increasing non-linearity and receptive field. This design principle is widely used in modern architectures like VGG.\n",
    "\n",
    "Thus, careful selection of filter size balances expressiveness and efficiency, while stride controls spatial resolution and computational load.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b27eb5",
   "metadata": {},
   "source": [
    "**Question 5:** Compare and contrast different CNN-based architectures like LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
    "\n",
    "**Answer:**  \n",
    "LeNet is one of the earliest CNN architectures, designed for handwritten digit recognition. It is shallow, with only a few convolutional layers and small filter sizes. It performs well on simple datasets like MNIST but lacks capacity for complex tasks.\n",
    "\n",
    "AlexNet marked a breakthrough in deep learning by winning the ImageNet competition. It introduced deeper architecture, ReLU activation, dropout, and GPU training. AlexNet uses larger filters (e.g., 11×11) in early layers and has significantly more parameters than LeNet.\n",
    "\n",
    "VGG networks further deepened CNNs by using uniform 3×3 filters throughout the architecture. This design increased depth while keeping parameters manageable. VGG achieved better performance and demonstrated that depth is crucial for representation learning.\n",
    "\n",
    "In summary, LeNet is simple and lightweight, AlexNet is deeper and more powerful, and VGG emphasizes depth and small filters for high performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d347b",
   "metadata": {},
   "source": [
    "**Question 6:** **Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb56632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rk\\tf_env\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 23ms/step - accuracy: 0.9578 - loss: 0.1383 - val_accuracy: 0.9858 - val_loss: 0.0500\n",
      "Epoch 2/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 22ms/step - accuracy: 0.9857 - loss: 0.0459 - val_accuracy: 0.9902 - val_loss: 0.0370\n",
      "Epoch 3/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 23ms/step - accuracy: 0.9899 - loss: 0.0309 - val_accuracy: 0.9908 - val_loss: 0.0311\n",
      "Epoch 4/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 22ms/step - accuracy: 0.9930 - loss: 0.0218 - val_accuracy: 0.9878 - val_loss: 0.0426\n",
      "Epoch 5/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 22ms/step - accuracy: 0.9948 - loss: 0.0168 - val_accuracy: 0.9918 - val_loss: 0.0342\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9895 - loss: 0.0348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03478849306702614, 0.9894999861717224]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1,28,28,1)/255.0\n",
    "x_test = x_test.reshape(-1,28,28,1)/255.0\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22977813-e36d-4504-a4b6-9e1cc12b1244",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c78249",
   "metadata": {},
   "source": [
    "**Question 7:**  **Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show your preprocessing and architecture.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26dd7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rk\\tf_env\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 25ms/step - accuracy: 0.4542 - loss: 1.5153 - val_accuracy: 0.5382 - val_loss: 1.2831\n",
      "Epoch 2/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 24ms/step - accuracy: 0.5918 - loss: 1.1634 - val_accuracy: 0.6208 - val_loss: 1.0938\n",
      "Epoch 3/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.6394 - loss: 1.0397 - val_accuracy: 0.6422 - val_loss: 1.0516\n",
      "Epoch 4/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 23ms/step - accuracy: 0.6672 - loss: 0.9583 - val_accuracy: 0.6562 - val_loss: 1.0052\n",
      "Epoch 5/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 24ms/step - accuracy: 0.6886 - loss: 0.8935 - val_accuracy: 0.6740 - val_loss: 0.9500\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6653 - loss: 0.9713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9713466763496399, 0.6653000116348267]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab2e610-b891-4d43-a279-7f3d43d888dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07abfce2",
   "metadata": {},
   "source": [
    "**Question 8:** **Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29049ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=True, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(64*5*5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64*5*5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = CNN()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e645e7-8ef5-4f6b-b00f-b94360b127c9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d364fe6f",
   "metadata": {},
   "source": [
    "**Question 9:** **Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2e12b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14055 images belonging to 1 classes.\n",
      "Found 3513 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    \"data\",\n",
    "    target_size=(224,224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    \"data\",\n",
    "    target_size=(224,224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4dbbc-7d9c-4c8a-8148-d38bb446a92d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c938d2a-fb2b-4cc0-9249-37a642d91746",
   "metadata": {},
   "source": [
    "**Question 10:** **You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into “Normal” and “Pneumonia” categories. Describe your end-to-end approach–from data preparation and model training to deploying the model as a web app using Streamlit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023ba0d",
   "metadata": {},
   "source": [
    "### problem statement:\n",
    "\n",
    "Chest X-Ray Pneumonia Detection Using CNN\n",
    "\n",
    "Pneumonia is a serious lung infection that can be life-threatening if not detected early. Chest X-ray imaging is one of the most common diagnostic tools used by radiologists. However, manual analysis is time-consuming and error-prone.\n",
    "\n",
    "The objective of this project is to build an end-to-end deep learning system that:\n",
    "\n",
    " - Classifies chest X-ray images into Normal and Pneumonia\n",
    " - Uses a Convolutional Neural Network (CNN)\n",
    " - Deploys the trained model as a web application using Streamlit\n",
    "\n",
    "This solution covers data preparation, model training, evaluation, and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6f055-bc65-4366-b1a0-294bb2f22088",
   "metadata": {},
   "source": [
    "**Dataset Organization**\n",
    "\n",
    "The dataset is organized into training, validation, and testing folders. Each folder contains two subfolders corresponding to class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b346706-a32c-4e0e-950a-28962ddff8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"small_data\"\n",
    "\n",
    "folders = [\n",
    "    \"train/NORMAL\",\n",
    "    \"train/PNEUMONIA\",\n",
    "    \"val/NORMAL\",\n",
    "    \"val/PNEUMONIA\"\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(os.path.join(base_dir, folder), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96848542-30fc-498a-b07e-0c354cb7095b",
   "metadata": {},
   "source": [
    "**Why Data Preprocessing is Important**\n",
    "\n",
    "Medical image datasets are often small and imbalanced. To improve generalization and avoid overfitting:\n",
    "\n",
    " - Images are resized to a fixed size\n",
    " - Pixel values are normalized\n",
    " - Data augmentation is applied\n",
    "\n",
    "Augmentation simulates real-world variations such as rotation and flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceed5049-acf8-4f4b-a397-0fa19d04bf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 390 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Train samples: 390\n",
      "Validation samples: 16\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    \"small_data/train\",\n",
    "    target_size=(224,224),\n",
    "    batch_size=16,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "val_data = val_datagen.flow_from_directory(\n",
    "    \"small_data/val\",\n",
    "    target_size=(224,224),\n",
    "    batch_size=16,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", train_data.samples)\n",
    "print(\"Validation samples:\", val_data.samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b013a-2a17-4560-ad01-e26fc2516b22",
   "metadata": {},
   "source": [
    "**Compilation Details**\n",
    "\n",
    "- Optimizer: Adam (fast convergence)\n",
    "- Loss Function: Binary Cross-Entropy (binary classification)\n",
    "- Metric: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6eb22d8-18b4-4992-9cff-6a851f7922da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa2ba07e-75dc-4e84-979d-a8ea90554a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.8051 - loss: 0.4486 - val_accuracy: 0.8125 - val_loss: 0.2974\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.9026 - loss: 0.2533 - val_accuracy: 0.9375 - val_loss: 0.3244\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.9282 - loss: 0.2002 - val_accuracy: 0.8750 - val_loss: 0.3337\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9128 - loss: 0.2215 - val_accuracy: 0.8125 - val_loss: 0.4741\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.9487 - loss: 0.1457 - val_accuracy: 0.8125 - val_loss: 0.4063\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9513 - loss: 0.1465 - val_accuracy: 0.7500 - val_loss: 0.3627\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9282 - loss: 0.1691 - val_accuracy: 0.7500 - val_loss: 0.4277\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9462 - loss: 0.1692 - val_accuracy: 0.8125 - val_loss: 0.5156\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9256 - loss: 0.1586 - val_accuracy: 0.8125 - val_loss: 0.3168\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9590 - loss: 0.1275 - val_accuracy: 0.8125 - val_loss: 0.5539\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70079b21-d1a0-40af-8a06-eb2ad0b67f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# saving the model\n",
    "model.save(\"pneumonia_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8d3378-c97e-48a8-a4a8-fa68d178f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the streamlit app file\n",
    "# app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3fbb28-2145-44a8-ab2a-7a2254949108",
   "metadata": {},
   "source": [
    "**Why Transfer Learning?**\n",
    "\n",
    "Training a CNN from scratch requires a large dataset. Medical datasets are usually limited.\n",
    "Hence, transfer learning is used.\n",
    "\n",
    "We use MobileNetV2, a lightweight and efficient CNN pre-trained on ImageNet.\n",
    "\n",
    "Benefits:\n",
    "\n",
    " - Faster convergence\n",
    " - Better accuracy\n",
    " - Reduced computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c87fb-7084-4dd2-9e86-7d39ac9debd6",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
