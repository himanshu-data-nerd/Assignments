{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf84d9e",
   "metadata": {},
   "source": [
    "## Can we use Bagging for regression problems?\n",
    "\n",
    "Yes, Bagging works for regression by training multiple regressors on bootstrap samples and averaging predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbe458",
   "metadata": {},
   "source": [
    "## Difference between multiple model training and single model training\n",
    "\n",
    "Single model uses one algorithm; multiple models combine several weak learners to reduce variance and improve stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce13086",
   "metadata": {},
   "source": [
    "## Feature randomness in Random Forest\n",
    "\n",
    "Each split considers a random subset of features, increasing diversity and reducing correlation between trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede5eef",
   "metadata": {},
   "source": [
    "## OOB (Out-of-Bag) Score\n",
    "\n",
    "Performance measured on samples not selected in bootstrap; acts like built-in cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4c643",
   "metadata": {},
   "source": [
    "## Feature importance in Random Forest\n",
    "\n",
    "Measured using Gini importance or permutation importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c46ceaa",
   "metadata": {},
   "source": [
    "## Working principle of Bagging Classifier\n",
    "\n",
    "Creates multiple bootstrap datasets, trains base estimators, and aggregates predictions by voting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d77952",
   "metadata": {},
   "source": [
    "## Evaluate a Bagging Classifier\n",
    "\n",
    "Use accuracy, precision, recall, F1-score, or confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85353ac5",
   "metadata": {},
   "source": [
    "## How a Bagging Regressor works\n",
    "\n",
    "Trains regressors on bootstrap samples and averages outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c2f06",
   "metadata": {},
   "source": [
    "## Main advantage of ensemble techniques\n",
    "\n",
    "Higher accuracy, reduced variance, and robust predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22e122",
   "metadata": {},
   "source": [
    "## Main challenge of ensemble methods\n",
    "\n",
    "High computation and less interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b23cd5",
   "metadata": {},
   "source": [
    "## Key idea behind ensemble techniques\n",
    "\n",
    "Combine weak learners to build a strong model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b180d7",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "Ensemble of decision trees using bagging and feature randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265dc95",
   "metadata": {},
   "source": [
    "## Types of ensemble techniques\n",
    "\n",
    "Bagging, Boosting, Stacking, Voting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9bdf7a",
   "metadata": {},
   "source": [
    "## Ensemble learning\n",
    "\n",
    "Combines multiple models for improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b70ea0",
   "metadata": {},
   "source": [
    "## When to avoid ensemble methods\n",
    "\n",
    "Avoid when data is small, model must be interpretable, or low compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48414b27",
   "metadata": {},
   "source": [
    "## How Bagging reduces overfitting\n",
    "\n",
    "Reduces variance via bootstrap aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b2a35",
   "metadata": {},
   "source": [
    "## Why Random Forest better than a single Decision Tree\n",
    "\n",
    "Lower variance and more stable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0034849c",
   "metadata": {},
   "source": [
    "## Role of bootstrap sampling in Bagging\n",
    "\n",
    "Creates diverse datasets for diverse models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328e1a8",
   "metadata": {},
   "source": [
    "## Real-world applications of ensemble techniques\n",
    "\n",
    "Fraud detection, credit scoring, recommendation, diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82f1ee",
   "metadata": {},
   "source": [
    "## Difference between Bagging and Boosting\n",
    "\n",
    "Bagging reduces variance with independent models; boosting reduces bias with sequential models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca636d72",
   "metadata": {},
   "source": [
    "## Bagging Classifier using Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3cbe8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a641a3",
   "metadata": {},
   "source": [
    "## Bagging Regressor – MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3504d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3863.819847191011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bdad5c",
   "metadata": {},
   "source": [
    "## Random Forest Classifier – Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835b1a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03073657 0.01832235 0.06103862 0.03736485 0.00798306 0.00452134\n",
      " 0.04425565 0.12000526 0.00262873 0.00299603 0.00959061 0.00402896\n",
      " 0.01655065 0.03035065 0.00256894 0.00426386 0.00550855 0.00654252\n",
      " 0.0034482  0.00599889 0.11662505 0.02262954 0.11892621 0.10548021\n",
      " 0.01172715 0.00922915 0.02476114 0.15365065 0.01042811 0.00783848]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5ab00b",
   "metadata": {},
   "source": [
    "## Random Forest Regressor vs Single Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b60e82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree R2: -0.16717320093143462\n",
      "RF R2: 0.4987935418156928\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Tree R2:\", r2_score(y_test, tree.predict(X_test)))\n",
    "print(\"RF R2:\", r2_score(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe6751",
   "metadata": {},
   "source": [
    "## Compute OOB Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f47d29fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Score: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "rf = RandomForestClassifier(oob_score=True, bootstrap=True)\n",
    "rf.fit(X, y)\n",
    "\n",
    "print(\"OOB Score:\", rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb78c05",
   "metadata": {},
   "source": [
    "## Bagging Classifier with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "910a1499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = BaggingClassifier(SVC(), n_estimators=25)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bba0bb",
   "metadata": {},
   "source": [
    "## RF with different trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6727e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.8666666666666667\n",
      "50 0.8666666666666667\n",
      "100 0.8666666666666667\n",
      "200 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "for n in [10, 50, 100, 200]:\n",
    "    rf = RandomForestClassifier(n_estimators=n)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print(n, rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a21013-d1fe-425d-9ec6-c9a0bc51fcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
